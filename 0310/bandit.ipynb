{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jw4570/2024/blob/main/0310/bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuDaRw3aGf17"
      },
      "source": [
        "# 강화 학습 환경 복습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0qCAXPrHGf1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8bd10f-19b3-4ae1-ce17-014fc0956d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[classic-control]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.5.2)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2vY1zCFGf2A"
      },
      "source": [
        "### 강화 학습 문제를 직접 풀어낼 정책 정의\n",
        "\n",
        "강화 학습에서는 어떤 함수를 학습하고자 하는 걸까요? 에이전트 안에는 상태 관측값(입력)을 받고 그것을 앞으로 취해야 할 최적의 행동(출력)에 매핑하는 함수가 있습니다. 예를 들어, 미로 속 에이전트의 현재 상태가 $(2, 3)$ 좌표라면, 에이전트 안의 함수는 이 입력값을 \"오른쪽으로 이동\"이라는 출력값에 매핑하는 것이 될 수 있습니다. 이 함수를 $\\pi$라고 한다면, 아래와 같이 수식으로 쓸 수 있습니다.\n",
        "$$\n",
        "\\pi((2, 3)) = \\text{\"오른쪽으로 이동\"}\n",
        "$$\n",
        "강화 학습 용어로 이 함수를 정책(policy)이라고 부릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQVpDtbyGf2B"
      },
      "outputs": [],
      "source": [
        "def policy(state):\n",
        "    x_position = state[0]\n",
        "    x_velocity = state[1]\n",
        "\n",
        "    if x_velocity > 0:\n",
        "        action = 2\n",
        "    elif x_velocity < 0:\n",
        "        action = 0\n",
        "    else:\n",
        "        if x_position > -0.6:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = 2\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMAngzWEGf2C"
      },
      "source": [
        "### 강화 학습이 돌아가는 환경의 코드 복습\n",
        "\n",
        "1. 인공지능 모델은 환경의 현재 상태(state)를 관찰할 수 있습니다. 미로 찾기 문제에서 환경의 현재 상태란 미로 속 현재 위치를 의미합니다. 예를 들어, 모델이 미로의 $(2, 3)$ 좌표에 있다면, 이 좌표는 현재 상태를 나타냅니다.\n",
        "\n",
        "2. 인공지능 모델은 관찰된 상태로부터 앞으로 취할 행동(action)을 결정합니다. 양갈래 길 중에서 어디로 갈지 결정하는 것 등이 그 예시가 될 수 있습니다.\n",
        "\n",
        "3. 환경은 상태를 변경(transition)시키고 그 행동에 대한 보상(reward)을 생성합니다. 인공지능 모델은 그 상태와 보상을 다 받습니다. 미로 찾기 문제에서 환경의 변화란 인공지능 모델의 (앞선 결정에 따른) 미로 속 위치 변화를 의미합니다. 예를 들어, '오른쪽으로 이동' 행동을 취하면, 에이전트의 위치 좌표가 $(2, 3)$에서 $(2, 4)$로 바뀔 수 있습니다. 보상은 출구를 찾았을 때 주어지는 경품이나 막다른 길에 도달했을 때 받는 페널티 등을 생각해 볼 수 있습니다.\n",
        "\n",
        "4.  이 새로운 정보(환경의 변화와 이에 따른 보상)를 사용하여 인공지능은 그런 행동이 좋아 그걸 반복해야 하는지, 또는 좋지 않아 회피해야 하는지 결정할 수 있습니다. 완료될 때까지 (done) 이 관측-행동-보상 사이클은 계속됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3E1Gi4oZGf2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "3567aea1-fc3f-4f81-a4e3-20f29da5348b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [-0.5439309  0.       ]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'policy' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-411f9830f40e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chose action:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "state, _ = env.reset()\n",
        "print(\"Initial state:\", state)\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    action = policy(state)\n",
        "    print(\"Chose action:\", action)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(\"New state:\", state)\n",
        "    print(\"Reward:\", reward)\n",
        "\n",
        "print(\"Final state:\", state)\n",
        "print(\"Total reward:\", total_reward)\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74uFnDQTGf2D"
      },
      "source": [
        "### 환경 직접 만들어보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1132,
      "metadata": {
        "id": "qNRU-Ma0Gf2D"
      },
      "outputs": [],
      "source": [
        "class MyEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.observation_space = gym.spaces.Discrete(7, start=-3)\n",
        "        self.action_space = gym.spaces.Discrete(2)\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def reset(self):\n",
        "        state = 0\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.num_steps += 1\n",
        "\n",
        "        if action == 0:\n",
        "            next_state = state - 1\n",
        "        else:\n",
        "            next_state = state + 1\n",
        "\n",
        "        if next_state > 3:\n",
        "            next_state = 3\n",
        "        elif next_state < -3:\n",
        "            next_state = -3\n",
        "\n",
        "        reward = {\n",
        "            -3: 1,\n",
        "            -2: 1,\n",
        "            -1: 1,\n",
        "            0: 0,\n",
        "            1: -1,\n",
        "            2: -1,\n",
        "            3: 10\n",
        "        }[next_state]\n",
        "\n",
        "        done = self.num_steps >= 3\n",
        "        return next_state, reward, done, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1133,
      "metadata": {
        "id": "XHV_E2zzGf2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "54876e4e-6580-4c0e-8cfb-0d141e2bb03a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1133-d39ec6884da4>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1133-d39ec6884da4>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    state = ???\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "class StudentMDP(gym.Env):\n",
        "    def __init__(self):\n",
        "        # 0: 수업, 1: 야자, 2: 집, 3: 잘침, 4: 못침\n",
        "        self.observation_space = gym.spaces.Discrete(5)\n",
        "\n",
        "        # 0: 공부, 1: 딴짓, 2: 땡땡이, 3: 쇼츠보기, 4: 벼락치기, 5: 수면\n",
        "        self.action_space = gym.spaces.Discrete(6)\n",
        "\n",
        "    def reset(self):\n",
        "        state = ???\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        state = ???\n",
        "        reward = ???\n",
        "        done = ???\n",
        "        return state, reward, done, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI8OFlxmGf2E"
      },
      "source": [
        "# 슬롯머신 정복하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GckfImVaGf2E"
      },
      "source": [
        "### 슬롯머신 환경 구현하기\n",
        "\n",
        "각각의 밴딧 $i = 1, \\cdots, n$에 대해, 먼저 랜덤하게 실제 가치 $\\mu_{i} \\sim N(0, 1)$를 평균 0 분산 1인 표준정규분포에서 추출해 정해줍니다. 그리고 $i$번 째 밴딧을 고를 경우, 받을 수 있는 보상은 다음과 같이 정해줍니다:\n",
        "$$\n",
        "r_{t} = \\mu_{i} + \\varepsilon \\quad \\text{where} \\quad \\varepsilon \\sim N(0, 1)\n",
        "$$\n",
        "즉, $i$번 째 밴딧을 골랐을 때의 보상은, 그 밴딧의 실제 가치 $\\mu_{i}$에 랜덤한 표준 정규 노이즈 $\\varepsilon \\sim N(0, 1)$을 더한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.normal()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtnkntszHDVN",
        "outputId": "0505f209-d85b-4cea-de1d-a351a875dbfd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36274503352596943"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 747,
      "metadata": {
        "id": "a0KseiTnGf2E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "class BanditEnv(gym.Env):\n",
        "    def __init__(self, num_bandits):\n",
        "        self.num_bandits = num_bandits # 슬롯머신의 갯수\n",
        "        self.action_space = list(range(num_bandits)) # 0~n-1번째 슬롯머신 선택 가능\n",
        "        self.observation_space = [0]\n",
        "\n",
        "    def reset(self):\n",
        "        # self.mean = np.random.normal(size=self.num_bandits) * 10\n",
        "        # 보상을 정해둠\n",
        "        self.mean = [8, 9, 7.5, 7, 8.5, 7, 6, 7.5, 8, 8.5]\n",
        "        return 0\n",
        "\n",
        "    def step(self, action):\n",
        "        state = 0 # 슬롯머신의 상태가 없음 > 항상 같음\n",
        "        mean = self.mean[action]\n",
        "        reward = mean + np.random.normal()\n",
        "        done = False\n",
        "        return state, reward, done, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86WbivkKTWi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 가중치로 구현"
      ],
      "metadata": {
        "id": "kk4y58dnoDKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "safety = 10 # 기대값이 작은수는 선택X 큰수 선택O\n",
        "\n",
        "def weights(x):\n",
        "    res = x[:]\n",
        "    MIN = min(res)\n",
        "    for i in range(len(res)):\n",
        "        res[i] -= MIN # 모든 수를 양수로 만듦\n",
        "        res[i] = res[i]**safety + 1 # 선택 확률이 0이 되지 않게 +1\n",
        "    return res\n",
        "\n",
        "class MyPolicy2():\n",
        "    def __init__(self, num_bandits):\n",
        "        self.num_bandits = num_bandits # 술롯머신 개수 저장\n",
        "        self.s = [0]*num_bandits # 보상의 합\n",
        "        self.n = [0]*num_bandits # 슬롯머신 선택 횟수\n",
        "        self.q = [0]*num_bandits # s/n (기대값)\n",
        "        self.total = 0 # 총합\n",
        "\n",
        "    def __call__(self, state, reward, action = -1):\n",
        "        if action == -1:\n",
        "            action = random.choices(list(range(self.num_bandits)), weights(self.q))[0]\n",
        "        return action\n",
        "\n",
        "    def setValue(self, action, reward):\n",
        "        self.s[action] += reward # 보상 합에 보상 추가\n",
        "        self.n[action] += 1 # 선택 횟수 추가\n",
        "        self.total += reward # 총합 추가\n",
        "        self.q[action] = self.s[action]/self.n[action] # 기댓값 계산"
      ],
      "metadata": {
        "id": "ZVwzNXrROZnJ"
      },
      "execution_count": 1076,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_bandits = 10 # 슬롯머신 개수\n",
        "choices = 10000 # 선택 횟수\n",
        "\n",
        "required = 1 # 모든 슬롯머신을 돌리는 것을 몇번 실행?\n",
        "\n",
        "history = {i: [] for i in range(env.num_bandits)}\n",
        "\n",
        "env = BanditEnv(num_bandits)\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "agent = MyPolicy2(num_bandits)\n",
        "reward = 0\n",
        "\n",
        "for _ in range(required):\n",
        "    for action in range(num_bandits):\n",
        "        agent(state, reward)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        agent.setValue(action, reward)\n",
        "\n",
        "        history[action].append(reward)\n",
        "\n",
        "for _ in range(choices-num_bandits*required):\n",
        "    action = agent(state, reward)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    agent.setValue(action, reward)\n",
        "\n",
        "    history[action].append(reward)\n",
        "    # print(\"Action:\", action, \"Reward:\", reward)\n",
        "\n",
        "print(\"Total reward:\", agent.total)\n",
        "print(\"보상 합:     \", agent.s)\n",
        "print(\"선택 횟수:   \", agent.n)\n",
        "print(\"기댓값:      \", agent.q)\n",
        "# print(positive(agent.q))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "for machine in range(env.num_bandits):\n",
        "    plt.scatter([machine]*len(history[machine]), history[machine])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "gED0NeRUKYk_",
        "outputId": "55d81216-b46d-41b8-ff76-00b1ee76fe69"
      },
      "execution_count": 1098,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 89416.17687639642\n",
            "보상 합:      [1022.4091518946237, 82263.61266078771, 7.129575657708464, 6.712668286872702, 6029.22307610256, 6.3303797031375, 6.423818548080709, 6.2905024936358584, 21.971364672977657, 46.073678249228934]\n",
            "선택 횟수:    [127, 9149, 1, 1, 710, 1, 1, 1, 3, 6]\n",
            "기댓값:       [8.050465762949793, 8.99154144286673, 7.129575657708464, 6.712668286872702, 8.491863487468395, 6.3303797031375, 6.423818548080709, 6.2905024936358584, 7.323788224325885, 7.678946374871489]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 그리드로 구현"
      ],
      "metadata": {
        "id": "sxJ8VcGSoPKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MyPolicy():\n",
        "    def __init__(self, num_bandits):\n",
        "        self.num_bandits = num_bandits # 술롯머신 개수 저장\n",
        "\n",
        "        # 처음 가치 추정치\n",
        "        initial_q = 100\n",
        "\n",
        "        self.q = [initial_q]*num_bandits # 평균값\n",
        "        self.n = [0]*num_bandits # 선택 횟수\n",
        "        self.epsilon = 0.01 # 랜덤 선택할 확률\n",
        "\n",
        "    def __call__(self, state):\n",
        "        action = [np.argmax(self.q), np.random.randint(self.num_bandits)][np.random.random() < self.epsilon] # epsilon보다 작을 경우 랜덤 선택\n",
        "        # action = np.argmax(self.q)\n",
        "\n",
        "        # argmax >\n",
        "        # max(list(range(self.num_bandits)), key = lambda x: self.q[x])\n",
        "        return action"
      ],
      "metadata": {
        "id": "04OSWNnYoS3F"
      },
      "execution_count": 1111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_bandits = 10 # 슬롯머신 개수\n",
        "choices = 10000 # 선택 횟수\n",
        "\n",
        "env = BanditEnv(num_bandits)\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "agent = MyPolicy(num_bandits)\n",
        "reward = 0\n",
        "\n",
        "cnt = [0]*env.num_bandits\n",
        "\n",
        "\n",
        "for _ in range(choices):\n",
        "    action = agent(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    # print(\"Action:\", action, \"Reward:\", reward)\n",
        "\n",
        "    agent.n[action] += 1\n",
        "    agent.q[action] += (reward - agent.q[action]) / agent.n[action] # 평균값을 데이터 효율적으로 업데이트 및 저장\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"Total reward:\", total_reward)\n",
        "# print(\"보상 합:     \", agent.s)\n",
        "print(\"선택 횟수:   \", agent.n)\n",
        "print(\"기댓값:      \", agent.q)\n",
        "# print(positive(agent.q))\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# for machine in range(env.num_bandits):\n",
        "#     plt.scatter([machine]*(len(history[machine])-1), history[machine][1:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3QBygn-oVFt",
        "outputId": "64d91577-9a5b-45d1-c1b5-cb5c149682f5"
      },
      "execution_count": 1131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 89628.1114146473\n",
            "선택 횟수:    [7, 9433, 8, 15, 473, 11, 7, 13, 12, 21]\n",
            "기댓값:       [7.685348257272362, 8.997317859188176, 7.770685951745193, 6.899087400193046, 8.566823017071934, 6.611320632685022, 6.031593697774013, 7.595951832584423, 7.824718508447007, 8.44123079351201]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl4tDuMxGf2F"
      },
      "source": [
        "### 슬롯머신 풀어보기\n",
        "\n",
        "직접 위 환경과 상호작용하면서, 각 슬롯머신의 보상을 추정해보세요. 그리고 Python을 이용해 위 문제에서 $\\epsilon=0.1$과 $\\epsilon=0.01$에 대해 각각 $\\epsilon$-greedy 방법을 적용해 보세요. (초기값 $Q_{1}(i)$는 모두 0으로 설정.) 또한 $Q_{1}(i) = 5$인 완전 탐욕적인 알고리즘 ($\\epsilon=0$) 또한 Python으로 구현하여 그 결과를 비교해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 814,
      "metadata": {
        "id": "a5Pc0yv7Gf2F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "2ba4f30c-d7d2-4c5c-d0fc-2f021f0a7388"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-814-5ed2b62abef5>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-814-5ed2b62abef5>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    action = ????\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class EpsilonGreedyPolicy:\n",
        "    def __init__(self, num_bandits, epsilon):\n",
        "        self.num_bandits = num_bandits\n",
        "        self.epsilon = epsilon\n",
        "        ????\n",
        "\n",
        "    def select_action(self):\n",
        "        action = ????\n",
        "        return action\n",
        "\n",
        "\n",
        "env = BanditEnv(10)\n",
        "policy = EpsilonGreedyPolicy(10, 0.1)\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "for t in range(100):\n",
        "    action = policy.select_action()\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(\"Action:\", action, \"Reward:\", reward)\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"Total reward:\", total_reward)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rllib",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}